---
layout: post
title: Interesting Papers III - "Language Models as Knowledge Bases?"
published: true
---
An overview of the paper “[Language Models as Knowledge Bases?](https://arxiv.org/abs/1909.01066)”, presented at EMNLP 2019 by researchers at Facebook AI Research and University College London.
<!--break-->
All images and tables in this post are from their paper.

## Motivation
Recently, pretrained language models like BERT have become the basis of state-of-the-art approaches for downstream NLP tasks (supervised learning tasks that utilize pretrained models). These pretrained models are generally trained on large corpora that contain vasts amount of information.

The authors of this paper aim to explore the extent to which existing “off-the-shelf” pretrained language models capture factual relational knowledge (like `{Moscow, capital-of, Russia}`), and compare their capabilities with structured knowledge graphs.

## The LAMA Probe
The authors introduce the LAnguage Model Analysis (LAMA) probe -- a set of knowledge sources, each containing a corpus of facts. Here, a fact is either a subject-relation-object triple or a question-answer pair. Each fact is converted to a cloze statement which is used to query the language model. The image below illustrates how a knowledge base or a language model would be queried for a fact using a relation triple or a Q/A pair respectively.

![How a Knowledge Graph would query a fact vs how a Language Model would](https://raw.githubusercontent.com/vamsi-aribandi/vamsi-aribandi.github.io/master/images/IP_III/querying_KB_vs_LM.png)

The following knowledge sources were considered:
* [Google-RE](https://code.google.com/p/relation-extraction-corpus/) (facts from Wikipedia),
* [T-REx](https://hadyelsahar.github.io/t-rex/) (facts from Wikipedia),
* [ConceptNet](http://conceptnet.io/) (commonsense relationships from OMCS), and
* [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) (a popular question answering dataset based on Wikipedia).

Each language model is evaluated on each knowledge source using the aforementioned cloze statements. These statements are either based on templates (Google-RE and T-REx), taken from the knowledge base’s source (ConceptNet) or manually constructed (SQuAD).

## Experimental Design
The following “off-the-shelf” language models were considered:
* [fairseq-fconv](https://github.com/pytorch/fairseq),
* [Transformer-XL](https://github.com/kimiyoung/transformer-xl),
* [ELMo](https://allennlp.org/elmo) (original and 5.5B), and
* [BERT](https://github.com/google-research/bert) (base and large).

The following baselines were considered to compare with language models:
* Freq (picks the most-frequently appearing object for a given relation),
* [RE](https://www.aclweb.org/anthology/D17-1188/) (a pretrained relation extraction and knowledge base construction algorithm), and
* [DrQA](https://github.com/facebookresearch/DrQA) (a popular system for question answering, considered only for SQuAD).

Mean precision at k (*P@k*) was considered as the evaluation metric (i.e., 1 if the correct answer is in the top k results, 0 otherwise).

### Miscellaneous design choices
* The choice of manually defined templates has an impact on the results (e.g., “[*mask*] is the capital of Russia” and “Russia’s capital is [*mask*]” might both give different results). The authors argue that due to this, they are measuring a lower bound for language models since they can be queried with many possible templates.
* The authors generally exclude relations involving multi-token objects from every knowledge source, which are not supported in their evaluation. This is because multi-token decoding would introduce numerous complexities and obscure the knowledge that is to be measured.
* A common vocabulary is considered for all the language models, which is the intersection of all models’ vocabularies.

## Results
The table below shows the mean *P@1* of the experiments conducted and statistics for the various knowledge sources.

![Results and statistics](https://raw.githubusercontent.com/vamsi-aribandi/vamsi-aribandi.github.io/master/images/IP_III/results.png)

For 1-1 relations in Google-RE and ConceptNet, BERT outperforms the other approaches. It also performs the best for the 1-1 relations in T-REx, but all language models perform poorly for N-M relations in T-REx. To investigate why BERT achieved stellar results, the authors computed correlations between *P@1* and various other statistics for T-REx, as can be seen below. For example, log-probability is positively correlated with *P@1*. This means that BERT performs well when it is confident in a prediction. Similarly, performance is positively correlated with cosine similarity between subject and object vectors.

![BERT P@1 correlations with different metrics](https://raw.githubusercontent.com/vamsi-aribandi/vamsi-aribandi.github.io/master/images/IP_III/BERT_correlations.png)

To evaluate how language models respond to different ways of querying a fact, the authors sampled 100 facts per relation in T-REx and queried the language model using 10 mentions per fact. They found that on average, BERT and ELMo ranked the correct answer lower (better) than other language models and were less sensitive to different queries (they exhibited lower variance), but also note that this may be due to the larger corpus they are trained on.

![How different Language Models respond to a variety of query forms per fact](https://raw.githubusercontent.com/vamsi-aribandi/vamsi-aribandi.github.io/master/images/IP_III/rank_distribution_different_queries.png)

For SQuAD, the supervised DrQA model predictably outperforms other approaches. However, when BERT is compared with DrQA in terms of *P@10* instead of *P@1*, the gap is notably small (57.1 for BERT-large and 63.5 for DrQA).

## Personal Opinions
In my opinion, the experiments conducted and the questions raised by the authors are important, but it would also have been nice to see an exploration of specific cases where language models provided absurdly wrong answers. [Some adversarial attacks on these language models](https://twitter.com/gneubig/status/1177276621172150272) from the perspective of relational knowledge would be very interesting to see.
Also, although the LAMA probe makes sense to evaluate language models, I feel that there is scope for a more comprehensive probe to be developed where current language modeling approaches perform poorly -- thus opening up a new NLP task to be solved. This is because even though BERT performs great on the probe, I do not think language models would be considered a viable alternative to structured knowledge sources anytime soon due to their unpredictability.