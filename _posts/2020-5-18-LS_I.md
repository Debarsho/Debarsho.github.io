---
layout: post
title: Literature Survey I - Knowledge Infusion in Language Models
published: true
---

A literature survey exploring recent works that attempt to augment language models using external knowledge. I will only include studies published after BERT to keep this post concise, as well as to draw attention to state-of-the-art-solutions.
<!--break-->
The studies will be discussed in reverse chronological order (my favourite ones are 1, 3, 4, 5, 8, and 9). All figures have been taken from their respective papers.

## Table of contents
1. [Weakly Supervised Knowledge-Pretrained LM - ICLR 2020](#weakly_supervised_knowledge_PLM)
2. [K-BERT - AAAI 2020](#K-BERT)
3. [LRLM - AAAI 2020](#LRLM)
5. [KnowBERT - EMNLP 2019](#KnowBERT)
4. [Giving BERT a calculator - EMNLP 2019](#BERT_calculator)
6. [KGLM - ACL 2019](#KGLM)
7. [COMET - ACL 2019](#COMET)
8. [Matching the Blanks - ACL 2019](#matching_blanks)
9. [ERNIE - ACL 2019](#ERNIE)
10. [KALM - NAACL 2019](#KALM)

***

## <a name="weakly_supervised_knowledge_PLM"></a> [Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model](https://openreview.net/forum?id=BJlzm64tDH) - UC Santa Barbara, Facebook AI

![Weakly Supervised Knowledge PLM](https://raw.githubusercontent.com/vamsi-aribandi/vamsi-aribandi.github.io/master/images/LS_I/weak_supervised_knowledge_PLM.png)
Xiong et al. propose a weakly supervised training objective to incorporate knowledge. The task requires the model to distinguish between true and false textual knowledge by replacing entity mentions in the original text with names of other entities of the same type before training a model to distinguish the correct entity mention from randomly chosen ones. The advantage of this model compared to other methods that try to infuse knowledge in language models is that it can directly do so via unstructured text, and doesn’t need any modifications to BERT while finetuning on downstream tasks. This study tests the proposed model on Question Answering and fine-grained Entity Typing -- two tasks that require entity knowledge -- and displays strong results for both.

***

## <a name="K-BERT"></a> [K-BERT: Enabling Language Representation with Knowledge Graph](https://arxiv.org/abs/1909.07606) - Peking University, Tencent Research

![K-BERT](https://raw.githubusercontent.com/vamsi-aribandi/vamsi-aribandi.github.io/master/images/LS_I/K-BERT.png)
Liu et al. propose a knowledge-enabled language representation model (K-BERT) which aims to overcome the issue of knowledge noise -- when injecting too much knowledge into a sentence changes its meaning. Initially, K-BERT “injects knowledge from KG into a sentence, making it a knowledge-rich sentence tree”. Next, it uses soft-position and visible matrix to “control the scope of knowledge, preventing it from deviating from its original meaning”. K-BERT outperforms BERT in several tasks where domain knowledge is essential like medicine, finance, and law. It should be noted that most of the experiments conducted by this paper are on Chinese datasets.

***

## <a name="LRLM"></a> [Latent Relation Language Models](https://arxiv.org/abs/1908.07690) - Carnegie Mellon

![LRLM](https://raw.githubusercontent.com/vamsi-aribandi/vamsi-aribandi.github.io/master/images/LS_I/LRLM.png)
Hayashi et al. propose Latent Relation Language Models (LRLMs: “a class of language models that parameterizes the joint distribution over the words in a document and the entities that occur therein via knowledge graph relations”. LRLMs model \\(P(X, Z | C)\\), where \\(X\\) is some textual data; \\(Z\\) is a sequence of latent variables that decides whether to generate words from a fixed word vocabulary or through spans defined according to their relations with a topic entity of interest; and \\(C\\) is the context from a knowledge base. The authors use [Latent Predictor Networks](https://www.aclweb.org/anthology/P16-1057/) as a model to learn this distribution and [WikiFacts](https://bitbucket.org/skaasj/wikifact_filmactor/src/master/) and [WikiText-103](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/) as datasets for evaluation. “Experiments demonstrate empirical improvements over both a word-based baseline language model and a previous approach that incorporates knowledge graph information”.

***

## <a name="KnowBERT"></a> [Knowledge Enhanced Contextual Word Representations](https://www.aclweb.org/anthology/D19-1005/) - Allen AI, University of Washington, UC Irvine

![KAR](https://raw.githubusercontent.com/vamsi-aribandi/vamsi-aribandi.github.io/master/images/LS_I/KAR.png)
Peters et al. propose the knowledge-enhanced BERT (KnowBERT), which integrates WordNet and a subset of Wikipedia through a novel Knowledge Attention and Recontextualization component (KAR). KAR accepts as input the contextual representations at a particular layer and computes knowledge enhanced representations. These representations are fed into the rest of the model as usual in BERT. KnowBERT “demonstrates improved perplexity, ability to recall facts as measured in a probing task and downstream performance on relationship extraction, entity typing, and word sense disambiguation”. KnowBERT also has a comparable runtime to BERT, as it scales to large knowledge bases.

***

## <a name="BERT_calculator"></a> [Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension](https://www.aclweb.org/anthology/D19-1609/) - Google Research
![Giving BERT a calculator - model operations](https://raw.githubusercontent.com/vamsi-aribandi/vamsi-aribandi.github.io/master/images/LS_I/BERT_calculator.png)
Andor et al. enable BERT to do lightweight numerical reasoning by augmenting it with a predefined set of executable programs. “Rather than having to learn to manipulate numbers directly, the model can pick a program and execute it”. The study mainly uses the [DROP dataset](https://allennlp.org/drop) for its evaluations and shows significant improvement by adding such shallow programs, but also shows that the proposed model performs well on the [Illinois dataset of math problems](https://cogcomp.seas.upenn.edu/page/resource_view/98) and the [CoQA](https://stanfordnlp.github.io/coqa/) dataset.

***

## <a name="KGLM"></a> [Barack’s Wife Hillary: Using Knowledge Graphs for Fact-Aware Language Modeling](https://www.aclweb.org/anthology/P19-1598/) - UC Irvine, Allen AI

![KGLM](https://raw.githubusercontent.com/vamsi-aribandi/vamsi-aribandi.github.io/master/images/LS_I/KGLM.png)
Logan et al. introduce the Knowledge Graph Language Model (KGLM), which has “mechanisms for selecting and copying information from an external knowledge graph”. The authors’ proposed model “maintains a dynamically growing local knowledge graph, a subset of the knowledge graph that contains entities that have already been mentioned in the text, and their related entities. When generating entity tokens, the model either decides to render a new entity that is absent from the local graph, thereby growing the local knowledge graph, or to render a fact from the local graph. When rendering, the model combines the standard vocabulary with tokens available in the knowledge graph, thus supporting numbers, dates, and other rare tokens”. The study uses the [Linked WikiText-2 dataset](https://rloganiv.github.io/linked-wikitext-2/#/) for its evaluations. Curiously, there are no mentions of transformers, BERT, or attention; and only uses LSTMs throughout.

***

## <a name="COMET"></a> [COMET: Commonsense Transformers for Automatic Knowledge Graph Construction](https://www.aclweb.org/anthology/P19-1470/) - Allen AI, University of Washington

![COMET](https://raw.githubusercontent.com/vamsi-aribandi/vamsi-aribandi.github.io/master/images/LS_I/COMET.png)
Bosselut et al. introduce the Commonsense Transformer (COMET), which follows their proposed generative approach to knowledge base construction: “A model must learn to produce new nodes and identify edges between existing nodes by generating phrases that coherently complete an existing seed phrase and relation type”. To do so, they fine-tune OpenAI’s [GPT](https://openai.com/blog/language-unsupervised/) by training it on a seed set of knowledge tuples. Human judges find that COMET is able to produce high-quality tuples on [Atomic](https://homes.cs.washington.edu/~msap/atomic/) and [ConceptNet](http://conceptnet.io/) (two knowledge bases), approaching human performance.

***

## <a name="matching_blanks"></a> [Matching the Blanks: Distributional Similarity for Relation Learning](https://www.aclweb.org/anthology/P19-1279/) - Google Research

![Matching the blanks](https://raw.githubusercontent.com/vamsi-aribandi/vamsi-aribandi.github.io/master/images/LS_I/matching_blanks.png)
Baldini Soares et al. propose a new training objective for learning relation representations directly from unstructured text. Although they propose a method that outperforms previous works for supervised relation extraction, their main contribution is a method of training such a representation without any supervision from a knowledge graph or human annotations. The training objective they introduce works by “matching the blanks”, which is based on the idea that relation statements that share the same two entities probably encode the same semantic relation. Specifically, the entities in both relations are replaced with a “blank” token with a certain probability and the model tries to classify whether two relations share the same meaning or not. Their proposed model achieves state-of-the-art results on various relation-extraction tasks and is particularly effective in low-resource settings.

***

## <a name="ERNIE"></a> [ERNIE: Enhanced Language Representation with Informative Entities](https://www.aclweb.org/anthology/P19-1139/) - Tsinghua University

![ERNIE](https://raw.githubusercontent.com/vamsi-aribandi/vamsi-aribandi.github.io/master/images/LS_I/ERNIE.png)
Zhang et al. train an enhanced language representation model (ERNIE). Their model uses algorithms like TransE to encode the graph structure or knowledge bases into a vector. After recognizing named entities, ERNIE “integrates entity representations in the knowledge module into the underlying layers of the semantic module”. On top of the masked language modeling and next sentence prediction training objectives proposed by BERT, the authors propose a new objective by “randomly masking some of the named entity alignments in the input text and asking the model to select appropriate entities from KGs to complete the alignments”. ERNIE performs comparably to BERT on common NLP tasks (i.e., the [GLUE](https://gluebenchmark.com/) benchmark) but achieves significant improvements on various knowledge-driven tasks.

***

## <a name="KALM"></a> [Knowledge-Augmented Language Model and Its Application to Unsupervised Named-Entity Recognition](https://www.aclweb.org/anthology/N19-1117/) - Facebook AI research

![KALM](https://raw.githubusercontent.com/vamsi-aribandi/vamsi-aribandi.github.io/master/images/LS_I/KALM.png)
Liu et al. propose the Knowledge-Augmented Language Model (KALM), which “works by providing a language model with the option to generate words from a set of entities from a database”. The model does this through a gating mechanism similar to attention in neural machine translation models. It outperforms popular baselines (which don’t include any transformer-based approaches) on the [recipe dataset](http://www.ffts.com/recipes.htm) and [CoNLL 2003](https://www.clips.uantwerpen.be/conll2003/ner/) in terms of perplexity. The authors also propose an unsupervised named entity recognizer that performs comparably to state-of-the-art supervised models.
